hello everyone welcome to this session
i'm mohan from simply learn and today
we'll talk about interview questions for
machine learning now this video will
probably help you when you're attending
interviews for machine learning
positions and the attempt here is to
probably consolidate 30 most commonly
asked questions and to help you in
answering these questions we tried our
best to give you the best possible
answers but of course what is more
important here is rather than the
theoretical
knowledge you need to kind of add to the
answers or supplement your answers with
your own experience so the responses
that we put here are a bit more generic
in nature so that if there are some
concepts that you are not clear this
video will help you in kind of getting
those concepts cleared up as well but
what is more important is that you need
to supplement these responses with your
own practical experience okay so with
that let's get started so one of the
first questions that you may face is
what are the different types of machine
learning now what is the best way to
respond to this there are three types of
machine learning if you read any
material you will always be told there
are three types of machine learning but
what is important is you would probably
be better off emphasizing that there are
actually two main types of machine
learning which is supervised and
unsupervised and then there is a third
type which is reinforcement learn so
supervised learning is where
you have some historical data and then
you feed that data to your model to
learn now you need to be aware of a
keyword that they will be looking for
which is labeled data right so if you
just say past data or historical data
the impact may not be so much you need
to emphasize on labeled data so what is
label data basically let's say if you
are trying to do train your model for
classification you need to be aware of
for your existing data which class each
of the observations belong to right so
that is what is labeling so it is
nothing but a fancy name you must be
already aware but just make it a point
to throw in that keyword labeled so that
will have the right impact okay so that
is what is supervised learning when you
have existing labeled data which you
then use to train your model that is
known as supervised learning and
unsupervised learning is when you don't
have this label data so you have data it
is not labeled so the system has to
figure out a way to do some analysis on
this okay so that is unsupervised
learning and you can then add a few
things like what are the ways of
performing uh supervised learning and
unsupervised learning and what are some
of the techniques so supervised learning
we we perform or we do
regression and classification and
unsupervised learning we do clustering
and clustering can be of different types
similarly regression can be of different
types but you don't have to probably
elaborate so much if they are asking
for
just the different types you can just
mention these and just at a very high
level but if they want you to elaborate
give examples then of course i think
there is a different question for that
we will see that later then the third so
we have supervised then we have
unsupervised and then reinforcement you
need to provide a little bit of
information around that as well because
it is sometimes a little difficult to
come up with a good definition for
reinforcement learning so you may have
to little bit elaborate on how
reinforcement learning works right so
reinforcement learning works in in such
a way that it basically has two parts to
it one is the agent and the environment
and the agent basically
is working inside of this environment
and it is given a target that it has to
achieve and uh every time it is moving
in the direction of the target so the
agent basically has to take some action
and every time he takes an action which
is moving
the agent towards the target right
towards a goal a target is nothing but a
goal
then it is rewarded and every time it is
going in a direction where it is away
from the goal then it is punished so
that is the way you can little bit
explain and
this is used primarily or very very
impactful for teaching the system to
learn games and so on examples of this
are basically used in alphago you can
throw that as an example where alphago
used reinforcement learning to actually
learn to play the game of go and finally
it defeated the co-world champion right
this much of information that would be
good enough okay then there could be a
question on overfitting uh so the
question could be what is overfitting
and how can you avoid it so
what is overfitting so let's first try
to understand the concept because
sometimes overfitting may be a little
difficult to understand overfitting is a
situation where the model has kind of
memorized the data so this is an
equivalent of memorizing the data so we
can draw an analogy so that it becomes
easy to explain this now let's say
you're teaching a child about
recognizing some fruits or something
like that okay and you're teaching this
child about recognizing let's say three
fruits apples oranges and pineapples
okay so this is a small child and for
the first time you're teaching the child
to recognize fruits then so what will
happen so this is very much like that is
your training data set so what you will
do is you will take a basket of fruits
which consists of apples oranges and
pineapples okay and
you take this basket to this child and
there may be let's say hundreds of these
fruits so you take this basket to this
child and keep showing each of this
fruit and then first time obviously the
child will not know what it is so you
show an apple and you say hey this is
apple then you show maybe an orange and
say this is orange and so on and so
forth and then again you keep repeating
that right so till that basket is over
this is basically how training work in
machine learning also that's how
training works so till the basket is
completed maybe 100 fruits you keep
showing this child in the process what
has happened the child has pretty much
memorized these so even before you
finish that basket right by the time you
are halfway through the child has learnt
about recognizing the apple orange and
pineapple now what will happen after
halfway through initially you remember
it made mistakes in recognizing but
halfway through now it has learned so
every time you show a fruit it will
exactly 100 accurately it will identify
it will say the child will say this is
an apple this is an orange and if you
show a pineapple it will say this is a
pineapple right so that means it has
kind of memorized this data now let's
say you bring another basket of fruits
and it will have a mix of maybe apples
which were already there in the previous
set but it will also have in addition to
apple it will probably have a banana or
maybe another fruit like a jackfruit
right so this is an equivalent of your
test data set which the child has not
seen before some parts of it it probably
has seen like the apples it has seen but
this banana and jackfruit it has not
seen so then what will happen in the
first round which is an equivalent of
your training data set towards the end
it has 100 it was telling you what the
fruits are right apple was accurately
recognized orange was accurately
recognized and pineapples were
accurately recognized right so that is
like 100 accuracy but now when you get
another a fresh set which we're not part
of the original one what will happen all
the apples maybe it will be able to
recognize correctly but all the others
like the jackfruit or the banana will
not be recognized by the child right so
this is an analogy this is an equivalent
of overfitting so what has happened
during the training process it is able
to recognize or reach 100 accuracy maybe
very high accuracy okay and we call that
as very low loss right so that is the
technical term so the loss is pretty
much zero and accuracy is pretty much
hundred percent whereas when you use
testing there will be a huge error which
means the loss will be pretty high and
therefore the accuracy will be also low
okay this is known as overfitting this
is basically a process where training is
done training processes it goes very
well almost reaching 100 accuracy but
while testing it really drops down now
how can you avoid it so that is the
extension of this question there are
multiple ways of avoiding overfitting
there are techniques like what you call
regularization that is the most common
technique that is used for
avoiding overfitting and within
regularization there can be a few other
subtypes like drop out in case of neural
networks and a few other examples but i
think if you give example or if you give
regularization as the technique probably
that should be sufficient so so there
will be some questions where the
interviewer will try to test your
fundamentals and your knowledge and
depth of knowledge and so on and so
forth and then there will be some
questions which are more like trick
questions that will be more to stump you
okay then the next question is around
the methodology so when we are
performing machine learning training we
split the data into training and test
right so this question is around that so
the question is what is training set and
test set in machine learning model and
how is the split done so the question
can be like that so in machine learning
when we are trying to train the model so
we have a three-step process we train
the model and then we test the model and
then once we are satisfied with the test
only then we deploy the model so what
happens in the train and test is that
you remember the labeled data so let's
say you have 1000 records with labeling
information now one way of doing it is
you use all the thousand records for
training and then maybe right which
means that you have exposed all these
thousand records during the training
process and then you take a small set of
the same data and then you say okay i
will test it with this okay and then you
probably what will happen you may get
some good results all right but there is
a flaw there what is the flaw this is
very similar to human beings it is like
you are showing this model the entire
data as a part of training okay so
obviously it has become familiar with
the entire data so when you are taking a
part of that again and you are saying
that i want to test it obviously you
will get good results so that is not a
very accurate way of testing so that is
the reason what we do is we have the
label data of this thousand records or
whatever we set aside before starting
the training process we set aside a
portion of the data and we call that
test set and the remaining we call as
training set and we use only this for
training our model now the training
process remember is not just about
passing one round of this data set so
let's say now your training set has 800
records it is not just one time you pass
this 800 records what you normally do is
you actually as a part of the training
you may pass this data through the model
multiple times so this thousand records
may go through the model maybe 10 15 20
times till the training is perfect till
the accuracy is high till the errors are
minimized okay now so which is fine
which means that your that is what is
known as the model has seen your data
and gets familiar with your data and now
when you bring your test data what will
happen is this is like some new data
because that is where the real test is
now you have trained the model and now
you are testing the model with some data
which is kind of new that is like a
situation like like a realistic
situation because when the model is
deployed that is what will happen it
will receive some new data not the data
that it has already seen right so this
is a realistic test so you put some new
data so this data which you have set
aside is for the model it is new and if
it is able to accurately predict the
values that means your training has
worked okay the model got trained
properly but let's say while you're
testing this with this test data you're
getting lot of errors that means you
need to probably either change your
model or retrain with more data and
things like that now coming back to the
question of how do you split this what
should be the ratio there is no fixed
number again this is like individual
preferences some people split it into 50
50 50 test and 50 training some people
prefer to have a larger amount for
training and a smaller amount for test
so they can go by either 60 40 or 70 30
or some people even go with some odd
numbers like 65 35 or
63.33 and 33 which is like one third and
two thirds so there is no fixed rule
that it has to be something the ratio
has to be this you can go by your
individual preferences all right then
you may have questions around
data handling data manipulation
or what you call data management or
preparation so these are all some
questions around that area there is
again no one answer one single good
answer to this it really varies from
situation to situation and depending on
what exactly is the problem what kind of
data it is how critical it is what kind
of data is missing and what is the type
of corruption so there are a whole lot
of things this is a very generic
question and therefore you need to be
little careful about responding to this
as well so probably have to illustrate
this again if you have experience in
doing this kind of work in handling data
you can illustrate with examples saying
that i was on one project where i
received this kind of data these were
the columns where data was not filled or
these were this many rows where the data
was missing that would be in fact a
perfect way to respond to this question
but if you don't have that obviously you
have to provide some good answer i think
it really depends on what exactly the
situation is and there are multiple ways
of handling the missing data or corrupt
data now let's take a few examples now
let's say you have data where some
values in some of the columns are
missing and you have pretty much half of
your data having these missing values in
terms of number of rows okay that could
be one situation another situation could
be that you have records or data missing
but
when you do some initial calculation how
many records are corrupt or how many
rows or observations as we call it has
this missing data let's assume it is
very minimal like 10
okay now
between these two cases how do we so
let's assume that this is not a mission
critical situation
and in order to fix this 10 percent of
the data the effort that is required is
much higher and obviously effort means
also time and money right so
it is not so mission critical and it is
okay to let's say get rid of these
records so obviously one of the easiest
ways of handling the data part or
missing data is remove those records or
remove those observations from your
analysis so that is the easiest way to
do but then the downside is as i said in
as in the first case if let's say 50
percent of your data is like that
because some column or the other is
missing so it is not like every in every
place in every row the same column is
missing but you have in maybe 10 percent
of the records column one is missing and
another ten percent column two is
missing another ten percent column three
is missing and so on and so forth so it
adds up to maybe half of your data set
so you cannot completely remove half of
your data set then the whole purpose is
lost okay so then how do you handle then
you need to come up with ways of filling
up this data with some meaningful value
right that is one way of handling so
when we say meaningful value what is
that meaningful let's say for a
particular column you might want to take
a mean value for that column and fill
wherever the data is missing fill up
with that mean value so that when you're
doing the calculations your analysis is
not completely way off so you have
values which are not missing first of
all so your system will work number two
these values are not so completely out
of whack that your whole analysis goes
for a task right there may be situations
where if the missing values instead of
putting mean may be a good idea to fill
it up with the minimum value or with a
zero so or with the maximum value again
as i said there are so many
possibilities so there is no like one
correct answer for this you need to
basically talk around this and
illustrate with your experience as i
said that would be the best otherwise
this is how you need to handle this
question okay so
then the next question can be how can
you choose a classifier based on a
training set data size so again this is
one of those questions
where you probably do not have like a
one-size-fits-all answer first of all
you
may not let's say
decide your classifier based on the
training set size maybe not the best way
to decide the type of the classifier and
even if you have to there are probably
some thumb rules which we can use but
then again every time so in my opinion
the best way to respond to this question
is you need to try out few classifiers
irrespective of the size of the data and
you need to then decide on your
particular situation which of these
classifiers are the right ones this is a
very generic issue so
you will never be able to just by if
somebody defines a problem to you and
somebody even if you if they show the
data to you or tell you what is the data
or even the size of the data i don't
think there is a way to really say that
yes this is the classifier that will
work here no that's not the right way so
you need to still
you know test it out get the data try
out a couple of classifiers and then
only you will be in a position to decide
which classifier to use you try out
multiple classifiers see which one gives
the best accuracy and only then you can
decide then you can have a question
around confusion matrix so the question
can be explained confusion matrix right
so confusion matrix i think the best way
to explain it is by taking an example
and drawing like a small diagram
otherwise it can really become tricky so
my suggestion is to take a piece of pen
and paper and
explain it by drawing a small matrix and
confusion matrix is about to find out
this is used especially in
classification
learning process and when you get the
results and our model predicts the
results you compare it with the actual
value and try to find out what is the
accuracy okay so in this case let's say
this is an example of a confusion matrix
and
it is a binary matrix so you have the
actual values which is the labeled data
right and which is so you have how many
yes and how many no so you have that
equation and you have the predicted
values how many yes and how many know
right so the total actual values the
total yes is 12 plus 113 and they are
shown here and the actual value those
are 9 plus 3 12 okay so that is what
this information here is so this is
about the actual and this is about the
predicted similarly the predicted values
there are yes are 12 plus 3 15 yeses and
no are 1 plus 9 10 nos okay so this is
the way to look at this confusion matrix
okay and
out of this what is the meaning convey
so there are two or three things that
needs to be explained outright the first
thing is for a model to be accurate the
values across the diagonal should be
high like in this case right that is one
number two the total sum of these values
is equal to the total observations in
the test data set so in this case for
example you have 12 plus 3 15 plus 10 25
so that means we have 25 observations in
our test data set okay so these are the
two things you need to first explain
that the total sum in this matrix the
numbers is equal to the size of the test
data set and the diagonal values
indicate the accuracy so by just by
looking at it you can probably have a
idea about is this uh an accurate model
is the model being accurate if they're
all spread out equally in all these four
boxes that means probably the accuracy
is not very good okay now how do you
calculate the accuracy itself right how
do you calculate the accuracy itself so
it is a very simple mathematical
calculation you take some of the
diagonals right so in this case it is 9
plus 12 21 and divide it by the total so
in this case what will it be let me take
a pen so your your diagonal values is
equal to if i say d is equal to 12 plus
9 so that is 21 right and the total data
set is equal to right we just calculated
it is 25 so what is your accuracy it is
21 by your accuracy is equal to 21 by 25
and this turns out to be about 85
percent right so this is 85 so that is
our accuracy okay so this is the way you
need to explain draw diagram give an
example and maybe it may be a good idea
to be prepared with an example so that
it becomes easy for you don't have to
calculate those numbers on the fly right
so a couple of uh hints are that you
take some numbers which are with which
add up to 100 that is always a good idea
so you don't have to really do this
complex calculations so the total value
will be 100 and then diagonal values you
divide once you find the diagonal values
that is equal to your percentage okay
all right so
the next question can be a related
question about
false positive and false negative so
what is false positive and what is false
negative now once again the best way to
explain this is using a piece of paper
and pen otherwise it will be pretty
difficult to explain this so we use the
same example of the confusion matrix
and
we can explain that so a confusion
matrix looks somewhat like this and
when we just yeah it looks somewhat like
this and we continue with the previous
example where this is the actual value
this is the predicted value and
in the actual value we have 12 plus 1 13
yeses and 3 plus 9 12 nos and the
predicted values there are 12 plus the
15 yeses and
one plus nine ten nos okay now this
particular case which is the false
positive what is a false positive first
of all the second word which is positive
okay is referring to the predicted value
so that means the system has predicted
it as a positive but the real value so
this is what the false comes from but
the real value is not positive okay that
is the way you should understand this
term false positive or even false
negative so false positive so positive
is what your system has predicted so
where is that system predicted this is
the one positive is what yes so you
basically consider this row okay now if
you consider this row so this is this is
all positive values this entire row is
positive values okay now the false
positive is the one which where the
value actual value is negative predicted
value is positive but the actual value
is negative so this is a false positive
right and here is a true positive so the
predicted value is positive and the
actual value is also positive okay i
hope this is making sense now let's take
a look at what is false negative false
negative so negative is the second term
that means that is the predicted value
that we need to look for so which are
the predicted negative values this row
corresponds to predicted negative values
all right so this row corresponds to
predicted negative values and what they
are asking for false so this is the row
for predicted negative values and the
actual value is this one right this is
predicted negative and the actual value
is also negative therefore this is a
true negative so the false negative is
this one predicted is negative but
actual is positive right so this is the
false negative so this is the way to
explain and this is the way to look at
false positive and false negative same
way there can be true positive and true
negative as well so again positive the
second term you will need to use to
identify the predicted row right so if
we say true positive positive we need to
take for the predicted part so predicted
positive is here okay and then the first
term is for the actual so true positive
so true in case of actual is yes right
so true positive is this one okay and
then in case of actual the negative now
we are talking about let's say true
negative true negative negative is this
one and the true comes from here so this
is true negative right 9 is true
negative the actual value is also
negative and the predicted value is also
negative okay so that is the way you
need to explain this the terms false
positive false negative and true
positive true negative then uh you might
have a question like what are the steps
involved in the machine learning process
or what are the three steps in the
process of developing a machine learning
model and so it is around the
methodology that is applied so basically
the way you can probably answer in your
own words but the way the model
development of the machine learning
model happens is like this so first of
all you try to understand the problem
and try to figure out whether it is a
classification problem or a regression
problem based on that you select a few
algorithms and then you start the
process of training these models okay so
you can either do that or you can after
due diligence you can probably decide
that there is one particular algorithm
that which is most suitable usually it
happens through trial and error process
but at some point you will decide that
okay this is the model we are going to
use okay so in that case we have the
model algorithm and the model decided
and then you need to do the process of
training the model and testing the model
and this is where if it is supervised
learning you split your data the label
data into training data set and test
data set and you use the training data
set to train your model and then you use
the test data set to check the accuracy
whether it is working fine or not so you
test the model before you actually put
it into production right so once you
test the model you're satisfied it's
working fine then you go to the next
level which is putting it for production
and then in production obviously new
data will come and
the inference happens so the model is
readily available and only thing that
happens is new data comes and the model
predicts the values whether it is
regression or classification now so this
can be an iterative process so it is not
a straightforward process where you do
the training through the testing and
then you move it to production now so
during the training and test process
there may be a situation where because
of either overfitting or things like
that the test doesn't go through which
means that you need to put that back
into the training process so that can be
an iterative process not only that even
if the training and test goes through
properly and you deploy the model in
production there can be a situation that
the data that actually comes the real
data that comes with that this model is
failing so in which case you may have to
once again go back to the drawing board
or initially it will be working fine but
over a period of time maybe due to the
change in the nature of the data once
again the accuracy will deteriorate so
that is again a recursive process so
once in a while you need to keep
checking whether the model is working
fine or not and if required you need to
tweak it and modify it and so on and so
forth so net net this is a continuous
process of
tweaking the model and testing it and
making sure it is up to date then you
might have question around deep learning
so because deep learning is now
associated with ai artificial
intelligence and so on so can be as
simple as what is deep learning so i
think the best way to respond to this
could be deep learning is a part of
machine learning and
then obviously the question would be
then what is the difference right so
deep learning you need to mention there
are two key parts that interviewer will
be looking for when you are defining
deep learning so first is of course deep
learning is a subset of machine learning
so machine learning is still the bigger
let's say uh scope and deep learning is
one part of it so then what exactly is
the difference deep learning is
primarily when we are implementing these
our algorithms or when we are using
neural networks for doing our training
and classification and regression and
all that right so when we use neural
network then it is considered as deep
learning and the term deep comes from
the fact that you can have several
layers of neural networks and these are
called deep neural networks and
therefore the term deep
deep learning the other difference
between machine learning and deep
learning which the interviewer may be
wanting to hear is that in case of
machine learning the feature engineering
is done manually what do we mean by
feature engineering basically when we
are trying to train our model we have
our training data right so we have our
training label data and
this data has several let's say if it is
a regular table it has several columns
now each of these columns actually has
information about a feature right so if
we are trying to predict the height
weight and so on and so forth so these
are all features of human beings let's
say we have census data and we have all
this so those are the features now there
may be probably 50 or 100 in some cases
there may be 100 such features now all
of them do not contribute to our model
right so we as a data scientist we have
to decide whether we should take all of
them all the features or we should throw
away some of them because again if we
take all of them number one of course
your accuracy will probably get affected
but also there is a computational part
so if you have so many features and then
you have so much data it becomes very
tricky so in case of machine learning we
manually take care of identifying the
features that do not contribute to the
learning process and thereby we
eliminate those features and so on right
so this is known as feature engineering
and in machine learning we do that
manually whereas in deep learning where
we use neural networks the model will
automatically determine which features
to use and which to not use and
therefore feature engineering is also
done automatically so this is a
explanation these are two key things
probably
will add value to your response all
right so the next question is what is
the difference between or what are the
differences between machine learning and
deep learning so here this is a quick
comparison table between machine
learning and deep learning and in
machine learning learning enables
missions to take decisions on their own
based on past data so here we are
talking primarily of supervised learning
and
it needs only a small amount of data for
training and then works well on low end
systems so you don't need large machines
and most features need to be identified
in advance and manually coded so
basically the feature engineering part
is done manually and the problem is
divided into parts and solved
individually and then combined so that
is about the machine learning part in
deep learning deep learning basically
enables machines to take decisions with
the help of artificial neural network so
here in deep learning we use neural nets
so that is the key differentiator
between machine learning and deep
learning and usually deep learning
involves a large amount of data and
therefore the training also requires
usually the training process requires
high-end machines because it needs a lot
of computing power and the machine
learning features are or the feature
engineering is done automatically so the
neural networks takes care of doing the
feature engineering as well and in case
of deep learning therefore it is said
that the problem is handled end-to-end
so this is a quick comparison between
machine learning and deep learning in
case you have that kind of a question
then you might get a question around the
uses of machine learning or some real
life applications of machine learning in
modern business the question may be
worded in different ways but the meaning
is how exactly is machine learning used
are actually supervised machine learning
it could be a very specific question
around supervised decision learning so
this is like give examples of supervised
machine learning use of supervised
machine learning in modern business so
that could be the next question so there
are quite a few examples or quite a few
use cases if you will for supervised
machine learning the very common one is
email spam detection so you want to
train your application or your system to
detect between spam and non-spam so this
is a very common business application of
supervised machine learning so how does
this work the way it works is that you
obviously have historical data about of
your emails and they are categorized as
spam and not spam so that is what is the
labeled information and then you feed
this information or the all these emails
as an input to your model right and the
model will then get trained to detect
which of the emails are to detect which
is spam and which is not spam so that is
the training process and this is
supervised machine learning because you
have label data you already have emails
which are tagged as spam or not spam and
then you use that to train your model
right so this is one example now there
are a few industry specific applications
for supervised machine learning one of
the very common ones is in healthcare
diagnostics in healthcare diagnostics
you have these images and you want to
train models to detect whether from a
particular image whether it can find out
if the person is sick or not whether a
person has cancer or not right so this
is a very good example of supervised
machine learning here the way it works
is that existing images it could be
x-ray images it'd be mri or any of these
images are available and they are tagged
saying that okay this x-ray image is
defective or the person has an illness
or it could be cancer whichever illness
right so it is tagged as a defective or
clear or good image and defective
something like that so we come up with a
binary or it could be multi-class as
well saying that this is defective to 10
percent this is 25 and so on but let us
keep it simple you can give an example
of just a binary classification that
would be good enough so
you can say that in healthcare
diagnostics using image we need to
detect whether a person is ill or
whether a person is having cancer or not
so here the way it works is you feed
labeled images and you allow the model
to learn from that so that when new
image is fed it will be able to predict
whether this person is having that
illness or not having cancer or not
right so i think this would be a very
good example for supervised machine
learning in modern business all right
then we can have a question like so
we've been talking about supervised and
unsupervised then so there can be a
question around semi-supervised machine
learning so what is semi-supervised
machine learning now semi-supervised
learning as the name suggests it falls
between supervised learning and
unsupervised learning but for all
practical purposes it is considered as a
part of supervised learning and the
reason this has come into existence is
that in supervised learning you need
labeled data so all your data for
training your model has to be labeled
now this is a big problem in many
industries or in many under many
situations getting the label data is not
that easy because there's a lot of
effort in labeling this data let's take
an example of the diagnostic images we
can just let's say take x-ray images now
there are actually millions of x-ray
images available all over the world but
the problem is they are not labeled so
their images are there but whether it is
effective or whether it is good that
information is not available along with
it right in a form that it can be used
by a machine which means that somebody
has to take a look at these images and
usually it should be like a doctor and
then say that okay yes this image is
clean and this image is cancerous and so
on and so forth now that is a huge
effort by itself so this is where
semi-supervised learning comes into play
so what happens is there is a large
amount of data maybe a part of it is
labeled then we try some techniques to
label the remaining part of the data so
that we get completely labeled data and
then we train our model so i know this
is a little long winding explanation but
unfortunately there is no
quick and easy definition for semi
supervised machine learning this is the
only way probably to explain this
concept
we may have another question as
what are unsupervised machine learning
techniques or what are some of the
techniques used for performing
unsupervised machine learning so it can
be worded in different ways so how do we
answer this question so unsupervised
learning you can say that there are two
types clustering and association and
clustering is a technique where similar
objects are put together and there are
different ways of finding similar
objects so their characteristics can be
measured and if they have in most of the
characteristics if they are similar then
they can be put together this is
clustering then association you can i
think the best way to explain
association is with an example in case
of association you try to find out how
the items are linked to each other so
for example if somebody bought a maybe a
laptop the person has also purchased a
mouse so this is more in an e-commerce
scenario for example so you can give
this as an example so people who are
buying and laptops are also buying the
mouse so that means there is an
association between laptops and mouse or
maybe people who are buying bread are
also buying butter so that is a
association that can be created so this
is unsupervised learning one of the
techniques okay all right then we have
very fundamental question what is the
difference between supervised and
unsupervised machine learning so
machine learning these are the two main
types of machine learning supervised and
unscienced and in case of supervised and
again here probably the key word that
the person may be wanting to hear is
labeled data very often people say we
have historical data and if we run it it
is supervised and if we don't have
historical data yes but you may have
historical data but if it is not labeled
then you cannot use it for supervised
learning so it is it's very key to
understand that we put in that keyword
labeled okay so when we have labeled
data for training our model then we can
use supervised learning and if we do not
have label data then we use unsupervised
learning and there are different
algorithms available to perform both of
these types of trainings so
there can be another question a little
bit more theoretical and conceptual in
nature this is about inductive machine
learning and deductive machine learning
so the question can be what is the
difference between inductive machine
learning and deductive machine learning
or somewhat in that manner so that the
exact phrase or exact question can vary
they can ask for examples and things
like that but that could be the question
so let's first understand what is
inductive and deductive training
inductive training is induced by
somebody and you can illustrate that
with a small example i think that always
helps so whenever you're doing some
explanation try as much as possible as i
said to give examples from your work
experience or give some analogies and
that will also help a lot in explaining
as well and for the interviewer also to
understand so here we'll take an example
or rather we will use an analogy so
inductive training is when we induce
some knowledge or the learning process
into a person without the person
actually experiencing it okay what can
be an example so we can probably tell
the person or show a person a video that
fire can burn the thing burn his finger
or fire can cause damage so what is
happening here this person has never
probably seen a fire
or never seen anything getting damaged
by fire but just because he has seen
this video he knows that okay fire is
dangerous and if a fire can cause damage
right so this is inductive learning
compared to that what is deductive
learning so here you draw a conclusion
or the person draws conclusion out of
experience so we will stick to the
analogy so compared to the showing a
video let's assume a person is allowed
to play with fire right and then he
figures out that if he puts his finger
it's burning or if throw something into
the fire it burns so he is learning
through experience so this is known as
deductive learning okay so you can have
applications or models that can be
trained using inductive learning or
deductive learning all right i think
probably that explanation will be
sufficient
the next question is are knn and k means
clustering similar to one another or are
they same right because that the letter
k is kind of common between them okay so
let us take a little while to understand
what these two are one is k nn
is completely different k means
clustering is completely different k nnn
is a classification process and
therefore it comes under supervised
learning whereas k means clustering is
actually a unsupervised okay when you
have k and n when you want to implement
k n which is basically k nearest
neighbors the value of k is a number so
you can say k is equal to 3 you want to
implement k n with k is equal to 3 so
which means that it performs the
classification in such a way that how
does it perform the classification so it
will take three nearest objects and
that's why it's called nearest neighbor
so basically uh based on the distance it
will try to find out its nearest objects
that are let's say three of the nearest
objects and then it will check whether
the class they belong to which class
right so if all three belong to one
particular class obviously this new
object is also classified as that
particular class but it is possible that
they may be from two or three different
classes okay so let's say they are from
two classes and then if they are from
two classes now usually you take a odd
number you assign an odd number too so
if there are three of them and two of
them belong to one class and then one
belongs to another class so this new
object is assigned to the class to which
the two of them belong now the value of
k is sometimes tricky whether should you
use three should you use five should you
use seven that can be tricky because the
ultimate classification can also vary so
it's possible that if you're taking k as
three the object is probably in one
particular class but if you take k is
equal to 5 maybe the object will belong
to a different class because when you
are taking three of them probably two of
them belong to class one and one belong
to class two whereas when you take five
of them it is possible that only two of
them belong to class one and three of
them belong to class two so which means
that this object will belong to class
two right so you see that so it is the
class allocation can vary depending on
the value of k now k means on the other
hand is a clustering process and it is
unsupervised where what it does is the
system will basically identify how the
objects are how close the objects are
with respect to some of their features
okay and but the similarity of course is
the the letter k and in case of k means
also we specify its value and it could
be three or five or seven there is no
technical limit as such but it can be
any number of clusters that you can
create okay so based on the value that
you provide the system will create that
many clusters of similar objects so
there is a similarity to that extent
that k is a number in both the cases but
actually these two are completely
different processes
we have what is known as naive bayes
classifier and people often get confused
thinking that naive bayes is the name of
the person who found this classifier or
who developed this classifier which is
not 100 true base is the name of the
person bays is the name of the person
but naive is not the name of the person
right so naive is basically an english
word and that has been added here
because of the nature of this particular
classifier naive bayes classifier is a
probability based classifier and
it makes some assumptions that
presence of one feature of a class is
not related to the presence of any other
feature of maybe other classes right so
which is not a very strong or not a very
what do you say accurate assumption
because these features can be related
and so on but even if we go with this
assumption this whole algorithm works
very well even with this assumption and
that is the good side of it but the term
comes from that so that is the
explanation that you can
then there can be question around
reinforcement learning it can be
paraphrased in multiple ways one could
be can you explain how a system can play
a game of chess using reinforcement
learning or it can be any game so the
best way to explain this is again to
talk a little bit about what
reinforcement learning is about and then
elaborate on that to explain the process
so first of all reinforcement learning
has an environment and an agent and the
agent is basically performing some
actions in order to achieve a certain
goal and this goals can be anything
either if it is related to game then the
goal could be that you have to score
very high score high value high number
or it could be that your number of lives
should be as high as possible don't lose
life so this could be some of them more
advanced examples could be for driving
the automotive industry self-driving
cars they actually also make use of
reinforcement learning to teach the car
how to navigate through the roads and so
on and so forth that is also another
example now how does it work so if the
system is basically there is an agent
and environment and every time the agent
takes a step or performs a task which is
taking it towards the goal the final
goal let's say to maximize the score or
to minimize the number of lives and so
on or minimize the deaths for example it
is rewarded and every time it takes a
step which goes against that code right
contrary or in the reverse direction it
is penalized okay so it is like a carrot
and a stick system now how do you use
this to create a game of chess or to
create a system to play a game of chess
now the way this works is and this could
probably go back to this alphago example
where alphago defeated a human champion
so the way it works is in reinforcement
learning the system is allowed for
example if in this case we are talking
about chess so we allow the system to
first of all watch playing a game of
chess so it could be with a human being
or it could be the system itself there
are computer games of chess right so
either this new learning system has to
watch that game or watch a human being
play the game
because this is reinforcement learning
is pretty much all visual so when you're
teaching the system to play a game the
system will not actually go behind the
scenes to understand the logic of your
software of this game or anything like
that it is just visually watching the
screen and then it learns okay so
reinforcement learning to a large extent
it works on that so you need to create a
mechanism whereby your model will be
able to watch somebody playing the game
and then you allow the system also to
start playing the game so it pretty much
starts from scratch
okay and as it moves forward it it's at
right at the beginning and the system
really knows nothing about the game of
chess okay so initially it is a clean
slate it just starts by observing how
you're playing so it will make some
random moves and keep losing badly but
then what happens is over a period of
time so you need to now allow the system
or you need to play with the system not
just one two three four or five times
but hundreds of times thousands of times
maybe even hundreds of thousands of
times and that's exactly how alphago has
done it played millions of games between
itself and the system right so for the
game of chess also you need to do
something like that you need to allow
the system to purchase and then learn on
its own over a period of repetitions so
i think you can probably explain it
after this much to this extent and it
should be
sufficient
now this is another question which is
again somewhat similar but here the size
is not coming into picture so the
question is how will you know which
machine learning algorithm to choose for
your classification problem now this is
not only classification problem it could
be a regression problem i would like to
generalize this question so if somebody
asks you how will you choose how will
you know which algorithm to use the
simple answer is there is no way you can
decide exactly saying that this is the
algorithm i am going to use in a variety
of situations there are some guidelines
like for example you will obviously
depending on the problem you can say
whether it is a classification problem
or a regression problem and then in that
sense you are kind of restricting
yourself to if it is a classification
problem there are you can only apply a
classification algorithm right to that
extent you can probably let's say limit
the number of algorithms but now within
the classification algorithms you have
decision trees you have svm you have
logistic regression is it possible to
outright say yes so for this particular
problem since you have explained this
now this is the exact algorithm that you
can use that is not possible okay so we
have to try out a bunch of algorithms
see which one gives us the best
performance and best accuracy and then
decide to go with that particular
algorithm so in machine learning a lot
of it happens through trial and error
there is no real possibility that
anybody can just by looking at the
problem or understanding the problem
tell you that okay in this particular
situation this is exactly the algorithm
that you should use then the questions
may be around application of machine
learning and this question is
specifically around how amazon is able
to recommend other things to buy so this
is around recommendation engine how does
it work how does the recommendation
engine work so this is basically the
question is all about so the
recommendation engine again works based
on various inputs that are provided
obviously something like uh you know
amazon
a website or e-commerce site like amazon
collects a lot of data around the
customer behavior who is purchasing what
and if somebody is buying a particular
thing they are also buying something
else so this kind of association right
so this is the unsupervised learning we
talked about they use this to associate
and link or relate items and that is one
part of it so they kind of build
association between items saying that
somebody buying this is also buying this
that is one part of it then they also
profile the users right based on their
age their gender their geographic
location they will do some profiling and
then when somebody is logging in and
when somebody is shopping kind of the
mapping of these two things are done
they try to identify obviously if you
have logged in then they know who you
are and your information is available
like for example your age may be your
agenda and where you're located what you
purchased earlier right so all this is
taken and the recommendation engine
basically uses all this information and
comes up with recommendations for a
particular user so that is how the
recommendation engine works all right
then the question can be something very
basic like when will you go for
classification versus regression right
when do you do classification instead of
regression or when you use
classification instead of regression now
yes so so this is basically going back
to the understanding of the basics of
classification and regression so
classification is used when you have to
identify or categorize things into
discrete classes so the best way to
respond to this question is to take up
some examples and use it otherwise it
can become a little tricky the question
may sound very simple but explaining it
can sometimes be very tricky in case of
regression we use of course there will
be some keywords that they will be
looking for so just you need to make
sure you use those keywords one is the
discrete values and other is the
continuous values so for regression if
we are trying to find some continuous
values you use regression whereas if
you're trying to find some discrete
values you use classification and then
you need to illustrate what are some of
the examples so classification is like
let's say there are images and you need
to put them into classes like cat dog
elephant tiger something like that so
that is a classification problem or it
can be that is a multi-class
classification problem it could be
binary classification problem like for
example whether a customer will buy or
he will not buy that is a classification
binary classification it can be in the
weather forecast area now weather
forecast is again combination of
regression and classification because on
the one hand you want to predict whether
it's going to rain or not that's a
classification problem that's a binary
classification right whether it's going
to rain or not rain however you also
have to predict what is going to be the
temperature tomorrow right now
temperature is a continuous value you
can't answer the temperature in a yes or
no kind of a response right so what will
be the temperature tomorrow so you need
to give a number which can be like 20
degrees 30 degrees or whatever right so
that is where you use regression one
more example is stock price prediction
so that is where again you will use
regression so these are the various
examples so you need to illustrate with
examples and make sure you include those
keywords like discrete and continuous so
the next question is more about a little
bit of a design related question to
understand your concepts and things like
that so it is how will you design a spam
filter so how do you basically design or
develop a spam filter so i think the
main thing here is he is looking at
probably understanding your concepts in
terms of what is the algorithm you will
use or what is your understanding about
difference between classification and
regression
and things like that right and the
process of course the methodology and
the process so the best way to go about
responding to this is we say that okay
this is a classification problem because
we want to find out whether an email is
a spam or not spam so that we can apply
the filter accordingly so first thing is
to identify what type of a problem it is
so we have identified that it is a
classification then the second step may
be to find out what kind of algorithm to
use now since this is a binary
classification problem logistic
regression is a very common very common
algorithm but however right as i said
earlier also we can never say that okay
for this particular problem this is
exactly the algorithm that we can use so
we can also probably try decision trees
or even support vector missions for
example svm so we will kind of list down
a few of these algorithms and we will
say okay we want to we would like to try
out these algorithms and then we go
about taking your historical data which
is the labeled data which are marked so
you will have a bunch of emails and then
you split that into training and test
data sets you use your training data set
to train your model that or your
algorithm that you have used or rather
the model actually
so
and you actually will have three models
let's say you are trying to test out
three algorithms so you will obviously
have three models so you need to try all
three models and test them out as well
see which one gives the best accuracy
and then you decide that you will go
with that model okay so training and
test will be done and then you zero in
on one particular model and then you say
okay this is the model will you use we
will use and then go ahead and implement
that or put that in production so that
is the way you design a spam file the
next question is about random forest so
what is random forest so this is a very
straightforward question however the
response you need to be again a little
careful while we all know what is random
forest explaining this can sometimes be
tricky so one thing is random forest is
kind of in one way it is an extension of
decision trees because it is basically
nothing but you have multiple decision
trees and
trees will basically you will use for
doing if it is classification mostly it
is classification you will use the trees
for classification and then you use
voting for finding that the final class
so that is the underlyings but how will
you explain this how will you respond to
this so first thing obviously we will
say that random forest is one of the
algorithms and the more important thing
that you need to probably the
interviewer is waiting to hear is
ensemble learner right so this is one
type of ensemble learner what is
ensemble learner ensemble learner is
like a combination of algorithms so it
is a learner which consists of more than
one algorithm or more than one maybe
models okay so in case of random forest
the algorithm is the same but instead of
using one instance of it we use multiple
instances of it and we use so in a way
that is a random forest is an ensemble
learner there are other types of
ensemble learners where we have like we
use different algorithms itself so you
have one maybe logistic regression and
the decision tree combined together and
so on and so forth or there are other
ways like for example splitting the data
in a certain way and so on so that's all
about ensembl we will not go into that
but random foreign i think the
interviewer will be happy to hear this
word ensemble learners and so then you
go and explain how the random forest
works so if the random forest is used
for classification then we use what is
known as a voting mechanism so basically
how does it work let's say your random
forest consists of 100 trees
and each observation you pass through
this forest and each observation let's
say it is a classification problem
binary classification 0 or 1 and you
have 100 trees now if 90 trees say that
it is a zero and ten of the trees say it
is a one you take the majority you may
take a vote and since ninety of them are
saying zero you classify this as zero
then you take the next observation and
so on so that is the way uh random
forest works for classification if it is
a regression problem it's somewhat
similar but only thing is instead of
vote what we will do is sorry in
regression remember what happens you
actually calculate a value right so for
example you are using regression to
predict the temperature and you have 100
trees and each tree obviously will
probably predict a different value of
the temperature they may be close to
each other but they may not be exactly
the same value so these 100 trees so how
do you now find the actual value the
output for the entire forest right so
you have outputs of individual trees
which are a part of this forest but then
you need to find the final output of the
forest itself so how do you do that so
in case of regression you take like an
average or the mean of all the 100 trees
right so this is also a way of reducing
the error so maybe if you have only one
tree and if that one tree makes a error
it is basically 100
wrong or 100 right right but if you have
on the other hand if you have a bunch of
trees you are basically mitigating that
reducing that error okay so that is the
way random forest works so the next
question is considering the long list of
machine learning algorithms how will you
decide on which one to use so once again
here there is no way to outright say
that this is the algorithm that we will
use for a given data set this is a very
good question but then the response has
to be like again there will not be a one
size fits all so we need to first of all
you can probably shorten the list in
terms of by saying okay whether it is a
classification problem or it is a
regression problem to that extent you
can probably
shorten the list because you don't have
to use all of them if it is a
classification problem you only can pick
from the classification algorithms right
so for example if it's a classification
you cannot use linear regression
algorithm or if it is a regression
problem you cannot use svm or maybe now
you can use svm but maybe a logistic
regression right so to that extent you
can probably shorten the list but still
you will not be able to 100
decide on saying that this is the exact
algorithm that i am going to use so the
way to go about is you choose a few
algorithms based on what the problem is
you try out your data you train some
models of these algorithms check which
one gives you the lowest error or the
highest accuracy and based on that you
choose that particular algorithm okay
all right then they can be questions
around bias and variance so the question
can be what is bias and variance in
machine learning uh so you just need to
give out a definition for each of these
for example a bias in machine learning
it occurs when the predicted values are
far away from the actual values so that
is a bias okay and whereas they are all
all the values are probably they are far
off but they are very near to each other
though the predicted values are close to
each other right while they are far off
from the actual value but they are close
to each other you see the difference so
that is bias and then the other part is
your variance now variance is when the
predicted values are all over the place
right so the variance is high that means
it may be close to the target but it is
kind of very scattered so the points the
predicted values are not close to each
other right in case of bias the
predicted values are close to each other
but they are not close to the target but
here they may be close to the target but
they may not be close to each other so
they are a little bit more scattered so
that is what in case of a variance okay
then the next question is about again
related to bias and variance what is the
trade-off between bias and variance yes
i think this is a interesting question
because these two are heading in
different directions so for example if
you try to minimize the bias variance
will keep going high and if you try to
minimize the variance bias will keep
going high and there is no way you can
minimize both of them so you need to
have a trade-off saying that okay this
is the level at which i will have my
bias and this is the level at which i
will have variance so the trade-off is
that pretty much attack you decide what
is the level you will tolerate for your
bias and what is the level you will
tolerate for variance and a combination
of these two in such a way that your
final results are not way off and having
a trade-off will ensure that the results
are consistent right so that is
basically the output is consistent and
which means that they are close to each
other and they are also accurate that
means they are as close to the target as
possible right so if either of these is
high then one of them will go off the
track define precision and recall now
again here i think it would be best to
draw a diagram and take the confusion
matrix and it is very simple the
definition is like a formula your
precision is true positive by true
positive plus false positive and your
recall is true positive by true positive
plus false negative okay so that's you
can just show it in a mathematical way
that's pretty much you know that can be
shown that's the easiest way to define
so the next question can be about
decision tree what is decision tree
pruning and why is it so
basically decision trees are really
simple to implement and understand but
one of the drawbacks of decision trees
is that it can become highly complicated
as it grows right and the rules and the
conditions can become very complicated
and this can also lead to overfitting
which is basically that during training
you will get 100 accuracy but when you
are doing testing you'll get a lot of
errors so that is the reason
pruning needs to be done so the purpose
or the reason for doing decision tree
pruning is to reduce overfitting or to
cut down on overfitting and what is
decision tree pruning it is basically
that you reduce the number of branches
because as you may be aware a tree
consists of the root node and then there
are several internal nodes and then you
have the leaf nodes now if there are too
many of these internal nodes that is
when you face the problem of overfitting
and pruning is the process of reducing
those internal nodes all right so the
next question can be what is logistic
regression uh so basically logistic
regression is
one of the techniques used for
performing classification especially
binary classification now there is
something special about logistic
regression and there are a couple of
things you need to be careful about
first of all the name is a little
confusing it is called logistic
regression but it is used for
classification so this can be sometimes
confusing so you need to probably
clarify that to the interviewer if it's
really you know if it is required and
they can also ask this like a trick
question right so that is one part
second thing is the term logistic has
nothing to do with the usual logistics
that we talk about but it is derived
from log so that the mathematical
derivation involves log and therefore
the name logistic regression so what is
logistic regression and how is it used
so logistic regression is used for
binary classification and the output of
a logistic regression is either a zero
or a one and it varies so it's basically
it calculates a probability between zero
and one and we can set a threshold that
can vary typically it is 0.5 so any
value above 0.5 is considered as 1 and
if the probability is below 0.5 it is
considered as 0. so that is the way we
calculate the probability or the system
calculates the probability and based on
the threshold it sets a value of zero or
one which is like a binary
classification zero or one okay then we
have a question around k nearest
neighbor algorithm so explain k nearest
neighbor algorithm so first of all what
is a k nearest neighbor algorithm this
is a classification algorithm so that is
the first thing we need to mention and
we also need to mention that the k is a
number it is an integer and this is
variable and we can define what the
value of k should be it can be 2 3 5 7
and usually it is an odd number so that
is something we need to mention
technically it can be even number also
but then typically it would be odd
number and we will see why that is okay
so based on that we need to classify
objects okay we need to classify objects
so again it will be very helpful to draw
a diagram you know if you are explaining
i think that will be the best way so
draw some diagram like this and let's
say we have three clusters or three
classes existing and now you want to
find for a new item that has come you
want to find out which class this
belongs to right so you go about as the
name suggests it you go about finding
the nearest neighbors right the points
which are closest to this and how many
of them you will find that is what is
defined by k now let's say our initial
value of k was five
so you will find the k the five nearest
data points so in this case as it is
illustrated these are the five nearest
data points but then all five do not
belong to the same class or cluster so
there are a one belonging to this
cluster one the second one belonging to
this cluster two three of them belonging
to this third cluster okay so how do you
decide that's exactly the reason we
should as much as possible try to assign
a odd number so that it becomes easier
to assign this so in this case you see
that the majority actually if there are
multiple classes then you go with the
majority so since three of these items
belong to this class we assign which is
basically the in in this case the green
or the tennis or the third cluster as i
was talking about right so we assign it
to this third class so in this case it
is uh that's how it is decided okay so k
nearest neighbor so first thing is to
identify the number of neighbors that
are mentioned as k so in this case it is
k is equal to five so we find the five
nearest points and then find out out of
these five which class has the maximum
number in that and and then the
new data point is assigned to that class
okay so that's pretty much how k nearest
neighbors work all right so that brings
us to the end of this module i hope you
enjoyed this interview questions and uh
hope you will be able to crack your next
interview if any of these questions come
up and in case you have other questions
that you would like to be answered
please feel free to put them down below
this video in the comment section and we
will try to maybe compile all of them
and create a new video out of that okay
with that i would like to sign off thank
you very much have a good day
hi there if you like this video
subscribe to the simply learn youtube
channel and click here to watch similar
videos to nerd up and get certified
click here
